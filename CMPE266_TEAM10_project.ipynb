{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9e10fc-e026-4df2-bdc4-43ce5e74fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c52403c-ead2-4fa2-aac0-0538a8f139b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:22:18 WARN Utils: Your hostname, Myungjongs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.6 instead (on interface en0)\n",
      "22/04/14 18:22:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yoonjung/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/14 18:22:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02031f75-30fc-4caa-8f06-9e951c50475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yoonjung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1575f047-a110-4f0a-9440-37521b593c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yoonjung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd5c36-df9d-48be-88eb-495ed12da0bf",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94921f-6441-434c-8349-23e20bf034e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.read_csv('IMDB_Dataset.csv')\n",
    "pd_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bd5b6c-5e58-4613-990e-5180c48e42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ function to remove load our CSV into a dataframe ************\n",
    "def load_csv_to_df(file_loc):\n",
    "  # n limits # of rows loaded\n",
    "\n",
    "  # File location and type\n",
    "    file_location = file_loc\n",
    "    file_type = \"csv\"\n",
    "\n",
    "    # CSV options\n",
    "    infer_schema = \"true\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "\n",
    "  # The applied options are for CSV files. For other file types, these will be ignored.\n",
    "      df = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .option(\"multiLine\",True) \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .load(file_location)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bdcf78-42dc-474f-9bbc-ef4ffa7a5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Function to preprocess our dataframe **********\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def preprocess_body(body):\n",
    "  # note: not HTML therefore beautiful soup not technically required\n",
    "  body = BeautifulSoup(body)\n",
    "  \n",
    "  # remove any urls\n",
    "    urls  =  body.find_all('a')\n",
    "    if len(urls) > 0: body.a.clear()\n",
    "  \n",
    "    # remove code snippets\n",
    "    codes = body.find_all('code')\n",
    "    if len(codes) > 0: body.code.clear()\n",
    "\n",
    "    # delete preformatted text\n",
    "    pres = body.find_all('pre')\n",
    "    if len(pres) > 0: body.pre.clear()\n",
    "\n",
    "    # start with our list of words\n",
    "    text = body.get_text()\n",
    "\n",
    "    # blank list we will append to\n",
    "    words = []\n",
    "\n",
    "    # make lowercase and strip whitespace\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'([^\\s\\w_]|_)+', '', text)\n",
    "\n",
    "    # snowball stemmer object\n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "    # tokenize into sentences\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "    # tokenize each sentance into words\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        if word in stop_en: continue  # remove stopwords\n",
    "        if len(word) < 3: continue  # remove words < 3 characters\n",
    "        if not word.isalpha(): continue  # remove numbers\n",
    "\n",
    "    words.append(snowball.stem(word))  # append stemmed version of word to list\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8f99ee-bfdf-48f5-b729-7cef25d53e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# convert sentiment into numerical values\n",
    "def make_target_vector_numerical(df):\n",
    "    sentiment_to_num = StringIndexer(inputCol='sentiment',outputCol='label')\n",
    "    model = sentiment_to_num.fit(df)\n",
    "    df = model.transform(df)\n",
    "    return df\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94087edb-0417-4e45-9637-0094276b68ef",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e20ba79-6c84-49ed-907a-147d4026a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's \"...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "|Phil the Alien is...| negative|\n",
      "|I saw this movie ...| negative|\n",
      "|So im not a big f...| negative|\n",
      "|The cast played S...| negative|\n",
      "|This a fantastic ...| positive|\n",
      "|Kind of drawn in ...| negative|\n",
      "|Some films just s...| positive|\n",
      "|This movie made i...| negative|\n",
      "|I remember this f...| positive|\n",
      "|An awful film! It...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imdb dataset\n",
    "file_location = \"IMDB_Dataset.csv\"\n",
    "df_ori = load_csv_to_df(file_location)\n",
    "df_ori.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a4a2b6-67e3-4055-aefc-f02a79e0ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the body\n",
    "df_ori = df_ori.withColumn('filtered_body', preprocess_body(df_ori['review'])) \n",
    "\n",
    "# convert sentiment into numerical values\n",
    "df_ori = make_target_vector_numerical(df_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22a956-5cad-43dd-ab10-0fa8ea532590",
   "metadata": {},
   "source": [
    "## Step 2: Create Numerical Feature Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeff1da-ab83-4967-85f0-d80eb7537e41",
   "metadata": {},
   "source": [
    "Create term frequency vector using HashingTF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7742087f-fa49-4358-80d5-0a43a7cf5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "# create term frequency vectors using HashingTF\n",
    "# used to have numFeatures=20...\n",
    "hashingTF = HashingTF(inputCol=\"filtered_body\", outputCol=\"rawFeatures_htf\")\n",
    "df_ori = hashingTF.transform(df_ori)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4348e-88c8-4720-91dc-5963ff7fe030",
   "metadata": {},
   "source": [
    "### 2.1 Create term frequency vectors using CountVectorizer instead of HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91f950ac-4e25-4720-90a5-a58a97d508e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered_body\", outputCol=\"rawFeatures_cv\")\n",
    "cvModel_ori = cv.fit(df_ori)\n",
    "df_ori = cvModel_ori.transform(df_ori).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de49f527-be05-44ef-843b-ee99869d2161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:36:03 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# apply the Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol=\"rawFeatures_cv\", outputCol=\"features_idf\")\n",
    "\n",
    "idfModel_ori = idf.fit(df_ori)\n",
    "\n",
    "df_ori = idfModel_ori.transform(df_ori).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695be26-4d26-4936-8f09-ca142da4813f",
   "metadata": {},
   "source": [
    "### 2.2 Create feature vector utilizing Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ce2f04-53a7-44f2-9f3f-21817926ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:41:45 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:41:47 WARN MemoryStore: Not enough space to cache rdd_88_0 in memory! (computed 260.1 MiB so far)\n",
      "22/04/14 18:41:47 WARN BlockManager: Persisting block rdd_88_0 to disk instead.\n",
      "22/04/14 18:41:54 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:41:57 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/14 18:41:57 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#Create Word2Vec Model with vector size = 100 and context size = 5\n",
    "word2Vec = Word2Vec(vectorSize=100, windowSize=5, inputCol=\"filtered_body\", outputCol=\"word2vec\")\n",
    "\n",
    "# creates word vectors\n",
    "w2vModel_ori = word2Vec.fit(df_ori)\n",
    "\n",
    "# average word vectors for each review into one review vector\n",
    "df_ori = w2vModel_ori.transform(df_ori).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2424d59-bad4-45c6-a83d-2b1c885e505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:44:33 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, sentiment: string, filtered_body: array<string>, label: double, rawFeatures_cv: vector, features_idf: vector, word2vec: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our 3 cleaned dataframes to work with\n",
    "df_ori.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dea535-df17-4d18-8365-a38f5943e614",
   "metadata": {},
   "source": [
    "## Step 3: Test our features using some models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f76555-7401-4391-a748-fcb8c82dbef9",
   "metadata": {},
   "source": [
    "In this step, we run our feature vectors through different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefae9f1-27a8-456b-a333-211f714cc8d3",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "980ae930-0dec-420d-b9a2-87fe23e5c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:50:57 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/14 18:50:57 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/14 18:50:57 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "#using CountVector\n",
    "dt_cv_df = df_ori.select(['label', 'rawFeatures_cv']).limit(1000).cache()\n",
    "\n",
    "#using CountVector and Inverse Document Frequency\n",
    "dt_idf_df = df_ori.select(['label', 'features_idf']).limit(1000).cache()\n",
    "\n",
    "#using Word2Vec\n",
    "dt_w2v_df = df_ori.select(['label', 'word2vec']).limit(1000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6205677c-5fd7-4133-93f0-2df5981808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 18:59:24 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:59:25 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:59:25 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "22/04/14 18:59:39 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 18:59:40 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 276.7 MiB so far)\n",
      "22/04/14 18:59:40 WARN BlockManager: Persisting block rdd_3653_0 to disk instead.\n",
      "22/04/14 18:59:42 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 18:59:42 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 18:59:43 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 18:59:44 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 18:59:44 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 18:59:45 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 18:59:45 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 18:59:46 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 18:59:47 WARN MemoryStore: Not enough space to cache rdd_3653_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 18:59:47 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:59:48 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:59:48 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 18:59:48 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "22/04/14 19:00:01 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 19:00:02 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 276.7 MiB so far)\n",
      "22/04/14 19:00:02 WARN BlockManager: Persisting block rdd_3702_0 to disk instead.\n",
      "22/04/14 19:00:04 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 19:00:04 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 19:00:05 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 19:00:05 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 19:00:06 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 19:00:07 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 19:00:07 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 19:00:08 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/04/14 19:00:09 WARN MemoryStore: Not enough space to cache rdd_3702_0 in memory! (computed 416.2 MiB so far)\n",
      "22/04/14 19:00:10 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:10 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:10 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:10 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:11 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:11 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:11 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:11 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:00:12 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree count vectorizer accuracy: 0.6725432109764791\n",
      "Decision Tree TF-IDF accuracy: 0.6725432109764791\n",
      "Decision Tree Word2Vec accuracy: 0.7184921144324226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 19:00:12 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "def DecisionTreeAccuracy(df, label, feature, bins, depth):\n",
    "    dt = DecisionTreeClassifier(labelCol=label, featuresCol=feature, maxBins=bins, maxDepth=depth, seed=42)\n",
    "    (training, testing) = df.randomSplit([0.7,0.3], seed=42)\n",
    "  \n",
    "    # fit our model to the training data\n",
    "    pred = dt.fit(training)\n",
    "\n",
    "    # apply model to test data\n",
    "    test_results = pred.transform(testing)\n",
    "    acc_eval = MulticlassClassificationEvaluator()\n",
    "    accuracy = acc_eval.evaluate(test_results)\n",
    "    return accuracy\n",
    "\n",
    "# determine accuracy of model using each type of feature\n",
    "dt_cv_acc = DecisionTreeAccuracy(dt_cv_df, \"label\", \"rawFeatures_cv\", 10, 5)\n",
    "dt_idf_acc = DecisionTreeAccuracy(dt_idf_df, \"label\", \"features_idf\", 10, 5)\n",
    "dt_w2v_acc = DecisionTreeAccuracy(dt_w2v_df, \"label\", \"word2vec\", 10, 5)\n",
    "print(f\"Decision Tree count vectorizer accuracy: {dt_cv_acc}\")\n",
    "print(f\"Decision Tree TF-IDF accuracy: {dt_cv_acc}\")\n",
    "print(f\"Decision Tree Word2Vec accuracy: {dt_w2v_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d2261-b8fc-44d8-bebd-a3f95a339855",
   "metadata": {},
   "source": [
    "**We see a slight edge with Word2Vec - since these models train quickly we can attempt some further tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea8a98-9777-4604-90e2-d1bc47450607",
   "metadata": {},
   "source": [
    "we can try to apply different tuning in order to get best Decision Tree --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf87afa-f387-4970-b778-1c76ba3fb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# using cross validation and Word2Vec\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train, test = dt_w2v_df.randomSplit([0.7,0.3], seed=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"word2vec\", seed=42)\n",
    "  \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .addGrid(dt.maxBins, [5, 10, 15])\\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15, 20])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3, seed=42)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    " \n",
    "print(\"Impurity parameter: \")\n",
    "print(bestModel._java_obj.getImpurity())\n",
    "print(\"maxDepth best param: \")\n",
    "print(bestModel._java_obj.getMaxDepth())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbeef0cd-46e5-4866-8cba-3bb7671c49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to give the accuracy, f1 score, precision, recall, and confusion matrix for a model on a specified dataframe. \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def model_evaluation(df, model, dataSourceName):\n",
    "    (training, testing) = df.randomSplit([0.7,0.3], seed=42)\n",
    "  \n",
    "    # fit our model to the training data\n",
    "    pred = model.fit(training)\n",
    "\n",
    "    # apply model to test data\n",
    "    test_results = pred.transform(testing)\n",
    "  \n",
    "    # print out accuracy\n",
    "    acc_eval = MulticlassClassificationEvaluator()\n",
    "    accuracy = acc_eval.evaluate(test_results)\n",
    "\n",
    "    # other metrics\n",
    "    predictionAndLabel = test_results.select(\"prediction\", \"label\").rdd\n",
    "    multiMetrics = MulticlassMetrics(predictionAndLabel)\n",
    "    precision = multiMetrics.weightedPrecision\n",
    "    recall = multiMetrics.weightedRecall\n",
    "    f1 = multiMetrics.weightedFMeasure()\n",
    "    matrix = multiMetrics.confusionMatrix().toArray()\n",
    "  \n",
    "    print(f\"Using model on {dataSourceName} data\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(\"Confustion Matrix: \")\n",
    "    print(matrix)\n",
    "    print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "917fee62-7334-48c3-bbd6-3a188026391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/14 19:05:16 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:18 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:19 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:21 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:21 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:21 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:22 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:22 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:22 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:23 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n",
      "22/04/14 19:05:25 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model on report data\n",
      "Accuracy: 0.7634236158029044\n",
      "Precision: 0.7634395444323038\n",
      "Recall: 0.7634322373696872\n",
      "F1: 0.7634236158029044\n",
      "Confustion Matrix: \n",
      "[[5644. 1800.]\n",
      " [1740. 5780.]]\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "dt_tuned_w2v = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"word2vec\", maxBins=16, maxDepth=10, seed=42, impurity=\"entropy\")\n",
    "dt_df_ori = df_ori.select(['label', 'word2vec']).cache()\n",
    " \n",
    "model_evaluation(dt_df_ori, dt, \"report\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
